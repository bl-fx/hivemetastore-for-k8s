apiVersion: v1
kind: ConfigMap
metadata:
  name: hivemetastore-hive-metastore
  labels:
    {{- include "hivemetastore-for-k8s.labels" . | nindent 4 }}
data:
  core-site.xml: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>fs.s3a.access.key</name>
            <value>access_key</value>
        </property>
        <property>
            <name>fs.s3a.secret.key</name>
            <value>secret_key</value>
        </property>
        <property>
            <name>fs.s3a.connection.ssl.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>fs.s3a.endpoint</name>
            <value>url_for_s3</value>
        </property>
        <property>
            <name>fs.s3a.path.style.access</name>
            <value>true</value>
        </property>
        <property>
            <name>fs.s3a.impl</name>
            <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>
        </property>
        <property>
            <name>fs.s3a.aws.credentials.provider</name>
            <value>org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider</value>
        </property>
    </configuration>
  hive-log4j2.properties: >-
    # Licensed to the Apache Software Foundation (ASF) under one

    # or more contributor license agreements.  See the NOTICE file

    # distributed with this work for additional information

    # regarding copyright ownership.  The ASF licenses this file

    # to you under the Apache License, Version 2.0 (the

    # "License"); you may not use this file except in compliance

    # with the License.  You may obtain a copy of the License at

    #

    #     http://www.apache.org/licenses/LICENSE-2.0

    #

    # Unless required by applicable law or agreed to in writing, software

    # distributed under the License is distributed on an "AS IS" BASIS,

    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.

    # See the License for the specific language governing permissions and

    # limitations under the License.


    status = DEBUG

    name = HiveLog4j2

    packages = org.apache.hadoop.hive.ql.log


    # list of properties

    property.hive.log.level = DEBUG

    property.hive.root.logger = DRFA

    property.hive.log.dir = ${sys:java.io.tmpdir}/${sys:user.name}

    property.hive.log.file = hive.log

    property.hive.perflogger.log.level = DEBUG


    # list of all appenders

    appenders = console, DRFA


    # console appender

    appender.console.type = Console

    appender.console.name = console

    appender.console.target = SYSTEM_ERR

    appender.console.layout.type = PatternLayout

    appender.console.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n


    # daily rolling file appender

    appender.DRFA.type = RollingRandomAccessFile

    appender.DRFA.name = DRFA

    appender.DRFA.fileName = ${sys:hive.log.dir}/${sys:hive.log.file}

    # Use %pid in the filePattern to append <process-id>@<host-name> to the
    filename if you want separate log files for different CLI session

    appender.DRFA.filePattern =
    ${sys:hive.log.dir}/${sys:hive.log.file}.%d{yyyy-MM-dd}

    appender.DRFA.layout.type = PatternLayout

    appender.DRFA.layout.pattern = %d{ISO8601} %5p [%t] %c{2}: %m%n

    appender.DRFA.policies.type = Policies

    appender.DRFA.policies.time.type = TimeBasedTriggeringPolicy

    appender.DRFA.policies.time.interval = 1

    appender.DRFA.policies.time.modulate = true

    appender.DRFA.strategy.type = DefaultRolloverStrategy

    appender.DRFA.strategy.max = 30


    # list of all loggers

    loggers = NIOServerCnxn, ClientCnxnSocketNIO, DataNucleus, Datastore, JPOX,
    PerfLogger, AmazonAws, ApacheHttp


    logger.NIOServerCnxn.name = org.apache.zookeeper.server.NIOServerCnxn

    logger.NIOServerCnxn.level = WARN


    logger.ClientCnxnSocketNIO.name = org.apache.zookeeper.ClientCnxnSocketNIO

    logger.ClientCnxnSocketNIO.level = WARN


    logger.DataNucleus.name = DataNucleus

    logger.DataNucleus.level = ERROR


    logger.Datastore.name = Datastore

    logger.Datastore.level = ERROR


    logger.JPOX.name = JPOX

    logger.JPOX.level = ERROR


    logger.AmazonAws.name=com.amazonaws

    logger.AmazonAws.level = DEBUG


    logger.ApacheHttp.name=org.apache.http

    logger.ApacheHttp.level = DEBUG


    logger.PerfLogger.name = org.apache.hadoop.hive.ql.log.PerfLogger

    logger.PerfLogger.level = ${sys:hive.perflogger.log.level}


    # root logger

    rootLogger.level = ${sys:hive.log.level}

    rootLogger.appenderRefs = root

    rootLogger.appenderRef.root.ref = ${sys:hive.root.logger}
  metastore-site.xml: |-
    <?xml version="1.0"?>
    <?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
    <configuration>
        <property>
            <name>metastore.task.threads.always</name>
            <value>org.apache.hadoop.hive.metastore.events.EventCleanerTask,org.apache.hadoop.hive.metastore.MaterializationsCacheCleanerTask</value>
        </property>
        <property>
            <name>metastore.expression.proxy</name>
            <value>org.apache.hadoop.hive.metastore.DefaultPartitionExpressionProxy</value>
        </property>
        <property>
            <name>metastore.warehouse.dir</name>
            <value>s3a://ware-house/</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionURL</name>
            <value>jdbc:postgresql://hivemetastore-postgresql/metastore</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionDriverName</name>
            <value>org.postgresql.Driver</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionUserName</name>
            <value>hive</value>
        </property>
        <property>
            <name>javax.jdo.option.ConnectionPassword</name>
            <value>hive</value>
        </property>
        <property>
            <name>datanucleus.autoCreateSchema</name>
            <value>false</value>
        </property>
    </configuration>
